# ğŸ¾ Pet Breed Classification via Architecture Fusion ğŸš€

This repository contains the implementation of a **Dual-Backbone Fusion Architecture** designed for fine-grained image classification using the Oxford-IIIT Pet Dataset.

Combining the structural depth of ResNet50 ğŸ—ï¸ with the textural efficiency of MobileNetV2 ğŸ“±, this model achieves high accuracy in distinguishing between 37 different breeds of cats and dogs.

## âœ¨ Features

* ğŸ§¬ **Dual-Backbone Fusion**: Late feature concatenation of ResNet50 and MobileNetV2
* ğŸ–¼ï¸ **Fine-Grained Accuracy**: Optimized for subtle differences between similar animal breeds
* âš¡ **Transfer Learning**: Leverages ImageNet pretrained weights for rapid convergence
* ğŸ“Š **Data-Driven Visuals**: Includes training loss/accuracy graphs and prediction galleries

## ğŸ› ï¸ Prerequisites

* ğŸ Python 3.12 is recommended for compatibility and performance
* A system with a GPU (CUDA) is preferred for faster training, but CPU is supported

## ğŸ Getting Started

### 1. ğŸ“¥ Clone the Repository

```bash
git clone https://github.com/mihkuno/IMPROCV.git
cd IMPROCV
```

### 2. ğŸ“¦ Install Dependencies

Ensure you have your virtual environment activated, then install the necessary packages:

```bash
pip install -r requirements.txt
```

### 3. ğŸ““ Run the Notebook

The entire pipelineâ€”from data download to training and inferenceâ€”is contained within the Jupyter Notebook:

```bash
jupyter notebook flower_fusion_case_study.ipynb
```

Once opened, select "Run All" to execute the data-driven analysis and view the visualizations. ğŸ’¡

## ğŸ“‚ Project Structure

* `flower_fusion_case_study.ipynb`: ğŸ“ The main notebook containing the model logic and visualizations
* `case_study_report.md`: ğŸ“– Formal LNCS-formatted report
* `data/`: ğŸ“ (Generated automatically) Stores the Oxford-IIIT Pet Dataset

## ğŸ“ Academic Credit

**Author**: JoeniÃ±o D. Cainday  
**Institution**: University of Science and Technology of Southern Philippines  
**Course**: CS412 Final Requirement  
**Date**: December 17, 2025
